{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision is one indicator of a machine learning model's performance â€“ the quality of a positive prediction made by the model. Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since we already know how to calulate true Postive and False postive, so we will use them to calulate Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true,y_pred):\n",
    "    # function to calculate true postive\n",
    "    tp=0\n",
    "    for yt,yp in zip(y_true,y_pred):\n",
    "        if (yt==1 and yp==1):\n",
    "            tp+=1\n",
    "    return tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive(y_true,y_pred):\n",
    "    # function to calculate true postive\n",
    "    fp=0\n",
    "    for yt,yp in zip(y_true,y_pred):\n",
    "        if (yt==0 and yp==1):\n",
    "            fp+=1\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(y_true,y_pred):\n",
    "    tp=true_positive(y_true,y_pred)\n",
    "    fp=false_positive(y_true,y_pred)\n",
    "    \n",
    "    precision=tp / (tp+fp)\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1=[0,1,1,1,0,0,0,1]\n",
    "l2=[0,1,0,1,0,1,0,0]\n",
    "\n",
    "Precision(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall is a measure of how many relevant elements were detected. Therefore it divides true positives by the number of relevant elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negative(y_true,y_pred):\n",
    "    # function to calculate true postive\n",
    "    fn=0\n",
    "    for yt,yp in zip(y_true,y_pred):\n",
    "        if (yt==1 and yp==0):\n",
    "            fn+=1\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(y_true,y_pred):\n",
    "    tp=true_positive(y_true,y_pred)\n",
    "    fn=false_negative(y_true,y_pred)\n",
    "    \n",
    "    recall=tp/(tp+fn)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1=[0,1,1,1,0,0,0,1]\n",
    "l2=[0,1,0,1,0,1,0,0]\n",
    "\n",
    "Recall(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score is a metric that combines both precision and recall. It is defined as a simple weighted average (harmonic mean ) of precision and recall\n",
    "### If we denote precision using P and recall using R, we can represent F1 Score as \n",
    "### F1= 2PR/(P+R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true,y_pred):\n",
    "    \n",
    "    p=Precision(y_true,y_pred)\n",
    "    r=Recall(y_true,y_pred);\n",
    "    \n",
    "    score=2*p*r/(p+r)\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=[0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0]\n",
    "y_pred=[0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
