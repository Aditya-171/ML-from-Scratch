## Mean Absolute Error
Absolute error refers to the magnitude of difference between the prediction of an observation and the true value of that observation. 
MAE( Mean Absolute error ) takes the average of absolute errors for a group of predictions and observations as a measurement of the magnitude of errors for the entire group
[MAE](https://github.com/Aditya-171/Photos/blob/master/download.png)

---

## Mean Squared Error

The Mean Squared Error (MSE) is perhaps the simplest and most common loss function taught.
To calculate the MSE, you take the difference between your model's predictions and the ground truth, square it, and average it out across the whole dataset. The mean squared error (MSE) tells you how close a regression line is to a set of points
[MSE](https://github.com/Aditya-171/Photos/blob/master/download%20(1).png)

---
## Root Mean Squared Error

Root mean squared error (RMSE) is the square root of the mean of the square of all of the error. 
[RMSE](https://github.com/Aditya-171/Photos/blob/master/download%20(2).png)

---

## R2 Score

The R2 score is a very important metric that is used to evaluate the performance of a regression-based machine learning model. 
It is pronounced as R squared and is also known as the coefficient of determination. 
It works by measuring the amount of variance in the predictions explained by the dataset. Simply put, it is the difference between the samples in the dataset and the predictions made by the model. Correlation (otherwise known as “R”) is a number between 1 and -1 where a value of +1 implies that an increase in x results in some increase in y, -1 implies that an increase in x results in a decrease in y, and 0 means that there isn’t any relationship between x and y. Like correlation, R² tells you how related two things are.
[R2](https://github.com/Aditya-171/Photos/blob/master/download%20(3).png)

---


## Adjusted R2 

Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.
Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts makers of the model to add even more variables. This is called overfitting and can return an unwarranted high R-squared value. Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.
[Adjusted R2](https://github.com/Aditya-171/Photos/blob/master/download%20(4).png)

---




